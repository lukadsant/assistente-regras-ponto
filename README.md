# ğŸ•’ Assistente de Regras de Ponto

Um assistente virtual inteligente que responde perguntas sobre as regras de ponto da empresa usando **IA local** (Llama3 via Ollama).

## ğŸ“‹ O que Ã© este projeto?

Este projeto Ã© um **chatbot especializado** que utiliza:
- **RAG (Retrieval Augmented Generation)**: Busca informaÃ§Ãµes especÃ­ficas no documento de regras
- **Ollama + Llama3**: Modelo de IA open-source rodando 100% localmente
- **LangChain**: Framework para construir aplicaÃ§Ãµes com LLMs
- **Streamlit**: Interface web simples e interativa

### ğŸ¯ Principais Vantagens

âœ… **Privacidade Total**: Todos os dados permanecem na sua mÃ¡quina  
âœ… **Respostas Precisas**: Baseadas no documento oficial de regras  
âœ… **FÃ¡cil de Usar**: Interface web intuitiva  
âœ… **Gratuito**: Sem custos com APIs de IA  

## ğŸš€ Como Funciona

```
ğŸ“„ Documento de Regras
    â†“
ğŸ”ª DivisÃ£o em chunks
    â†“
ğŸ§® GeraÃ§Ã£o de embeddings
    â†“
ğŸ’¾ Armazenamento vetorial (ChromaDB)
    â†“
â“ Pergunta do usuÃ¡rio
    â†“
ğŸ” Busca semÃ¢ntica (RAG)
    â†“
ğŸ¤– Llama3 gera resposta contextualizada
    â†“
ğŸ’¬ Resposta ao usuÃ¡rio
```

## ğŸ“¦ PrÃ©-requisitos

- **Python 3.10+**
- **Windows 10/11** (para Ollama no Windows)
- **8GB RAM** mÃ­nimo (16GB recomendado para Llama3)

## âš™ï¸ InstalaÃ§Ã£o

### 1ï¸âƒ£ Instalar o Ollama

**No Windows:**
```powershell
# OpÃ§Ã£o 1: Download direto
# Acesse: https://ollama.com/download
# Baixe e execute o instalador .exe

# OpÃ§Ã£o 2: Via winget (recomendado)
winget install --id=Ollama.Ollama
```

**No Linux/Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2ï¸âƒ£ Baixar o modelo Llama3

```powershell
# ApÃ³s instalar o Ollama, baixe o modelo
ollama pull llama3
```

> âš ï¸ **Nota**: O download do Llama3 pode levar alguns minutos (aproximadamente 4.7GB)

### 3ï¸âƒ£ Clonar o repositÃ³rio

```bash
git clone https://github.com/lukadsant/assistente-regras-ponto.git
cd assistente-regras-ponto
```

### 4ï¸âƒ£ Criar ambiente virtual e instalar dependÃªncias

```powershell
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente virtual
.venv\Scripts\Activate.ps1

# Instalar dependÃªncias
pip install -r requirements.txt
```

**Ou usando uv (mais rÃ¡pido):**
```powershell
uv sync
```

## ğŸ® Como Usar

### Iniciar a aplicaÃ§Ã£o

```powershell
streamlit run app.py
```

O navegador abrirÃ¡ automaticamente em `http://localhost:8501`

### Fazer perguntas

Na interface web, digite suas perguntas sobre as regras de ponto:

## ğŸ’¡ Exemplos de Uso

**Pergunta:** "Qual o horÃ¡rio de trabalho?"  
**Resposta:** "O horÃ¡rio de trabalho Ã© das 8h Ã s 17h, com 1 hora de almoÃ§o..."

**Pergunta:** "Posso compensar horas extras?"  
**Resposta:** "Sim, as horas extras podem ser compensadas dentro do mesmo mÃªs..."

**Pergunta:** "Qual a tolerÃ¢ncia para atrasos?"  
**Resposta:** "A tolerÃ¢ncia Ã© de 10 minutos por dia, atÃ© no mÃ¡ximo 3 vezes por semana..."

**Pergunta:** "Como funciona o banco de horas?"  
**Resposta:** "O banco de horas permite acumular horas extras que podem ser compensadas..."

## ğŸ“ Estrutura do Projeto

```
assistente-regras-ponto/
â”œâ”€â”€ app.py                 # AplicaÃ§Ã£o principal Streamlit
â”œâ”€â”€ main.py                # Script alternativo/testes
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ regras_ponto.txt   # Documento com regras da empresa
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ pyproject.toml         # ConfiguraÃ§Ã£o do projeto (uv)
â”œâ”€â”€ uv.lock               # Lock file do uv
â””â”€â”€ README.md             # Este arquivo
```

## ğŸ”§ PersonalizaÃ§Ã£o

### Alterar o documento de regras

Edite o arquivo `docs/regras_ponto.txt` com as regras especÃ­ficas da sua empresa.

### Trocar o modelo de IA

No arquivo `app.py`, altere o modelo:

```python
# Modelos disponÃ­veis: llama3, llama3.1, mistral, codellama, etc.
embeddings = OllamaEmbeddings(model="llama3.1")
llm = OllamaLLM(model="llama3.1")
```

Para listar modelos disponÃ­veis:
```bash
ollama list
```

### Ajustar configuraÃ§Ãµes do Streamlit

Para rodar em porta/endereÃ§o especÃ­fico:
```powershell
streamlit run app.py --server.port 7860 --server.address 0.0.0.0
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Ollama nÃ£o encontrado"

Certifique-se de que o Ollama estÃ¡ instalado e no PATH:
```powershell
ollama --version
```

Se nÃ£o funcionar, reinicie o terminal ou adicione ao PATH:
`C:\Users\SEU_USUARIO\AppData\Local\Programs\Ollama`

### Erro: "Model not found"

Baixe o modelo novamente:
```powershell
ollama pull llama3
```

### AplicaÃ§Ã£o lenta

- Verifique se tem RAM suficiente (mÃ­nimo 8GB)
- Considere usar um modelo menor: `ollama pull llama3:8b`
- Ajuste o `chunk_size` no cÃ³digo para valores menores

### ChromaDB warnings

SÃ£o avisos normais do SQLite. Podem ser ignorados ou desabilitados com:
```python
import warnings
warnings.filterwarnings('ignore')
```

## ğŸ› ï¸ Tecnologias Utilizadas

- [Streamlit](https://streamlit.io/) - Interface web
- [LangChain](https://python.langchain.com/) - Framework LLM
- [Ollama](https://ollama.com/) - ExecuÃ§Ã£o local de LLMs
- [Llama3](https://llama.meta.com/) - Modelo de linguagem
- [ChromaDB](https://www.trychroma.com/) - Banco de dados vetorial

## ğŸ“ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto. Sinta-se livre para usar e modificar.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

## ğŸ‘¤ Autor

**lukadsant**
- GitHub: [@lukadsant](https://github.com/lukadsant)

---

â­ Se este projeto foi Ãºtil, considere dar uma estrela no GitHub!
