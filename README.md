# ï¿½ Assistente Empresarial

Um assistente virtual inteligente que responde perguntas sobre regras, procedimentos e documentos da empresa usando **IA local** (Llama3 via Ollama).

## ğŸ“‹ O que Ã© este projeto?

Este projeto Ã© um **chatbot especializado** que utiliza:

- **RAG (Retrieval Augmented Generation)**: Busca informaÃ§Ãµes especÃ­ficas em mÃºltiplos documentos
- **Ollama + Llama3**: Modelo de IA open-source rodando 100% localmente
- **LangChain**: Framework para construir aplicaÃ§Ãµes com LLMs
- **Streamlit**: Interface web simples e interativa
- **Processamento automÃ¡tico de documentos**: Converte PDFs, DOCs e TXTs em base de conhecimento

### ğŸ¯ Principais Vantagens

âœ… **Privacidade Total**: Todos os dados permanecem na sua mÃ¡quina  
âœ… **Respostas Precisas**: Baseadas em mÃºltiplos documentos da empresa  
âœ… **Respostas em PortuguÃªs**: Configurado para responder sempre em portuguÃªs brasileiro  
âœ… **FÃ¡cil de Usar**: Interface web intuitiva  
âœ… **Gratuito**: Sem custos com APIs de IA  
âœ… **MÃºltiplos formatos**: Suporta PDF, DOC, DOCX e TXT

## ğŸš€ Como Funciona

### Pipeline de Processamento de Documentos

```
ï¿½ docs_raw/ (PDFs, DOCs, TXTs)
    â†“
ğŸ”„ build_dataset.py processa arquivos
    â†“
ğŸ¤– Llama3 gera resumos contextuais
    â†“
ğŸ“ docs_processed/ (arquivos processados)
```

### Pipeline de Busca e Resposta

```
ğŸ“‚ docs_processed/ (mÃºltiplos documentos)
    â†“
ğŸ”ª DivisÃ£o em chunks (500 caracteres)
    â†“
ğŸ§® GeraÃ§Ã£o de embeddings vetoriais
    â†“
ğŸ’¾ Armazenamento no ChromaDB
    â†“
â“ Pergunta do usuÃ¡rio (em portuguÃªs)
    â†“
ğŸ” Busca semÃ¢ntica (RAG) nos documentos
    â†“
ğŸ¤– Llama3 gera resposta em portuguÃªs
    â†“
ğŸ’¬ Resposta contextualizada ao usuÃ¡rio
```

## ğŸ“¦ PrÃ©-requisitos

- **Python 3.10+**
- **Windows 10/11** (para Ollama no Windows)
- **8GB RAM** mÃ­nimo (16GB recomendado para Llama3)

## âš™ï¸ InstalaÃ§Ã£o

### 1ï¸âƒ£ Instalar o Ollama

**No Windows:**

```powershell
# OpÃ§Ã£o 1: Download direto
# Acesse: https://ollama.com/download
# Baixe e execute o instalador .exe

# OpÃ§Ã£o 2: Via winget (recomendado)
winget install --id=Ollama.Ollama
```

**No Linux/Mac:**

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2ï¸âƒ£ Baixar o modelo Llama3

```powershell
# ApÃ³s instalar o Ollama, baixe o modelo
ollama pull llama3
```

> âš ï¸ **Nota**: O download do Llama3 pode levar alguns minutos (aproximadamente 4.7GB)

### 3ï¸âƒ£ Clonar o repositÃ³rio

```bash
git clone https://github.com/lukadsant/assistente-regras-ponto.git
cd assistente-regras-ponto
```

### 4ï¸âƒ£ Criar ambiente virtual e instalar dependÃªncias

```powershell
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente virtual
.venv\Scripts\Activate.ps1

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar pacotes adicionais necessÃ¡rios
pip install chardet  # Para detecÃ§Ã£o automÃ¡tica de encoding
```

**Ou usando uv (mais rÃ¡pido):**

```powershell
uv sync
```

### 5ï¸âƒ£ Preparar seus documentos

Crie as pastas necessÃ¡rias:

```powershell
# Criar estrutura de pastas
New-Item -ItemType Directory -Force -Path "docs_raw"
New-Item -ItemType Directory -Force -Path "docs_processed"
```

Coloque seus documentos (PDFs, DOCs, TXTs) na pasta `docs_raw/`:

```
docs_raw/
â”œâ”€â”€ regras_ponto.pdf
â”œâ”€â”€ manual_funcionario.docx
â”œâ”€â”€ procedimento_ferias.txt
â””â”€â”€ ...
```

### 6ï¸âƒ£ Processar os documentos

Execute o script de processamento:

```powershell
python build_dataset.py
```

Este script vai:
- âœ… Extrair texto de PDFs, DOCs e TXTs
- âœ… Gerar resumos contextuais usando Llama3
- âœ… Salvar os documentos processados em `docs_processed/`

## ğŸ® Como Usar

### 1. Iniciar a aplicaÃ§Ã£o

```powershell
streamlit run app.py
```

O navegador abrirÃ¡ automaticamente em `http://localhost:8501`

**Para rodar em rede local (acessÃ­vel de outros dispositivos):**

```powershell
streamlit run app.py --server.port 7860 --server.address 0.0.0.0
```

### 2. Verificar documentos carregados

Ao iniciar, a aplicaÃ§Ã£o mostra:
- âœ… Quantidade de documentos carregados
- âš ï¸ Avisos se nÃ£o houver documentos
- âŒ Erro se a pasta nÃ£o existir

### 3. Fazer perguntas

Na interface web, digite suas perguntas sobre procedimentos da empresa. **Todas as respostas sÃ£o em portuguÃªs brasileiro!**

## ğŸ’¡ Exemplos de Uso

### Perguntas sobre Regras de Ponto

**Pergunta:** "Qual o horÃ¡rio de trabalho?"  
**Resposta:** "O horÃ¡rio de trabalho Ã© das 8h Ã s 17h, com 1 hora de almoÃ§o..."

**Pergunta:** "Posso compensar horas extras?"  
**Resposta:** "Sim, as horas extras podem ser compensadas dentro do mesmo mÃªs..."

**Pergunta:** "Qual a tolerÃ¢ncia para atrasos?"  
**Resposta:** "A tolerÃ¢ncia Ã© de 10 minutos por dia, atÃ© no mÃ¡ximo 3 vezes por semana..."

### Perguntas sobre Procedimentos

**Pergunta:** "Como solicito fÃ©rias?"  
**Resposta:** "Para solicitar fÃ©rias, vocÃª deve preencher o formulÃ¡rio Form 80.02..."

**Pergunta:** "Qual o procedimento para abertura de vaga?"  
**Resposta:** "O procedimento envolve o preenchimento do Form 85.00 - Pedido de Abertura de Vaga..."

### Perguntas sobre FormulÃ¡rios

**Pergunta:** "Que formulÃ¡rios existem disponÃ­veis?"  
**Resposta:** "Temos diversos formulÃ¡rios, incluindo Form 85.00 para abertura de vagas..."

## ğŸ“ Estrutura do Projeto

```
assistente-regras-ponto/
â”œâ”€â”€ app.py                    # AplicaÃ§Ã£o principal Streamlit (RAG + Chat)
â”œâ”€â”€ build_dataset.py          # Script de processamento de documentos
â”œâ”€â”€ main.py                   # Script alternativo/testes
â”‚
â”œâ”€â”€ docs_raw/                 # ğŸ“¥ Documentos originais (PDFs, DOCs, TXTs)
â”‚   â”œâ”€â”€ regras_ponto.pdf
â”‚   â”œâ”€â”€ manual.docx
â”‚   â””â”€â”€ procedimento.txt
â”‚
â”œâ”€â”€ docs_processed/           # ğŸ“¤ Documentos processados (TXTs resumidos)
â”‚   â”œâ”€â”€ regras_ponto.txt
â”‚   â”œâ”€â”€ manual.txt
â”‚   â””â”€â”€ procedimento.txt
â”‚
â”œâ”€â”€ docs/                     # ğŸ“š Documentos legados (opcional)
â”‚   â””â”€â”€ regras_ponto.txt
â”‚
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ pyproject.toml            # ConfiguraÃ§Ã£o do projeto (uv)
â”œâ”€â”€ uv.lock                   # Lock file do uv
â””â”€â”€ README.md                 # Este arquivo
```

## ğŸ”§ PersonalizaÃ§Ã£o

### Adicionar novos documentos

1. Coloque os documentos em `docs_raw/`
2. Execute `python build_dataset.py`
3. Reinicie o Streamlit

### Trocar o modelo de IA

No arquivo `app.py`, altere o modelo:

```python
# Modelos disponÃ­veis: llama3, llama3.1, llama3.2, mistral, codellama, etc.
embeddings = OllamaEmbeddings(model="llama3.1")
llm = OllamaLLM(model="llama3.1")
```

Para listar modelos disponÃ­veis:

```bash
ollama list
```

Para baixar um modelo diferente:

```bash
ollama pull llama3.1
```

### Alterar o prompt do assistente

No arquivo `app.py`, edite a variÃ¡vel `prompt_template`:

```python
prompt_template = """VocÃª Ã© um assistente especializado em [SEU DOMÃNIO].
Use o contexto fornecido para responder perguntas sobre [SUA ÃREA]...
"""
```

### Ajustar tamanho dos chunks

Para documentos maiores ou menores, ajuste no `app.py`:

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Tamanho do chunk (padrÃ£o: 500)
    chunk_overlap=100  # SobreposiÃ§Ã£o (padrÃ£o: 50)
)
```

### Configurar porta/endereÃ§o do Streamlit

```powershell
streamlit run app.py --server.port 7860 --server.address 0.0.0.0
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Ollama nÃ£o encontrado"

Certifique-se de que o Ollama estÃ¡ instalado e no PATH:

```powershell
ollama --version
```

Se nÃ£o funcionar, reinicie o terminal ou adicione ao PATH:  
`C:\Users\SEU_USUARIO\AppData\Local\Programs\Ollama`

### Erro: "Model not found"

Baixe o modelo novamente:

```powershell
ollama pull llama3
```

### Erro: "No module named 'chardet'"

Instale o pacote necessÃ¡rio:

```powershell
pip install chardet
```

### Erro: "UnicodeDecodeError" ou "Error loading file"

O cÃ³digo jÃ¡ estÃ¡ configurado com `autodetect_encoding=True`, mas se persistir:

1. Verifique se o `chardet` estÃ¡ instalado
2. Tente reprocessar o documento com `build_dataset.py`
3. Salve manualmente o arquivo com encoding UTF-8

### Erro: "Pasta 'docs_processed' nÃ£o encontrada"

Execute o script de processamento primeiro:

```powershell
python build_dataset.py
```

### AplicaÃ§Ã£o lenta

- Verifique se tem RAM suficiente (mÃ­nimo 8GB)
- Considere usar um modelo menor: `ollama pull llama3:8b`
- Reduza o `chunk_size` no cÃ³digo
- Reduza o nÃºmero de documentos processados

### Respostas em inglÃªs

O cÃ³digo jÃ¡ estÃ¡ configurado para responder em portuguÃªs, mas se persistir:

1. Verifique se o `prompt_template` estÃ¡ correto no `app.py`
2. Adicione mais instruÃ§Ãµes explÃ­citas no prompt
3. Considere usar um modelo diferente

### ChromaDB warnings

SÃ£o avisos normais do SQLite. Podem ser ignorados ou desabilitados com:

```python
import warnings
warnings.filterwarnings('ignore')
```

## ğŸ› ï¸ Tecnologias Utilizadas

- [Streamlit](https://streamlit.io/) - Interface web interativa
- [LangChain](https://python.langchain.com/) - Framework para LLMs
- [Ollama](https://ollama.com/) - ExecuÃ§Ã£o local de LLMs
- [Llama3](https://llama.meta.com/) - Modelo de linguagem (Meta AI)
- [ChromaDB](https://www.trychroma.com/) - Banco de dados vetorial
- [PyPDF](https://pypdf.readthedocs.io/) - Processamento de PDFs
- [python-docx](https://python-docx.readthedocs.io/) - Processamento de DOCs
- [chardet](https://chardet.readthedocs.io/) - DetecÃ§Ã£o de encoding

## ğŸ“ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto. Sinta-se livre para usar e modificar.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

## ğŸ‘¤ Autor

**lukadsant**

- GitHub: [@lukadsant](https://github.com/lukadsant)

---

â­ Se este projeto foi Ãºtil, considere dar uma estrela no GitHub!
